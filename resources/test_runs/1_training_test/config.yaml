# User Parameters
# fast and dirty for testing
# Not for real training use!!!

DATA_DIR: inputs/czi  # folder for czi files
FORMAT: 2019 # 2019
I: [1,2,3,4] # all valid index options for czi file

WEIGHT: ../1_training_test/res/1_trained_weights/1.0.rep2.weights.h5

MIN_P: 0.1  # minimum probability threshold for classification, e.g. 0.5, 0.1, 0.01, 0.001







## Advanced params ##
# (Please don't change unless you're developer)
# This is a fast and dirty version for testing if the installation is correct, please don't use this file for actual gtraining of C-COUNT

# blob detection
test_mode: False  # only use small subset in blob detection for faster processing
block_height: 2000 # for block equalization
block_width: 2400  # if < 0, skip block equalization
blob_detection_scaling_factor: 4  # 1, 2, 4 (08/23/2021 for blob_detection)
max_sigma: 12 # 6 for 8 bit, larger for detecting larger blobs
min_sigma: 3  # 2-8
num_sigma: 20  # smaller->faster, less accurate, 5-20
threshold: 0.1  # 0.02 too sensitive, 0.1 to ignore debris
overlap: .2 # overlap larger than this, smaller blob gone, not sensitive

blob_extention_ratio: 1.4 # for vis in jpg
blob_extention_radius: 10 # for vis in jpg
crop_width: 80  # padding width, which is cropped img width/2 (50), in blob_cropping.py

# classification
# training and classification
BOOSTING: False
numClasses: 2  # if 2 is selected, 3[uncertain] 4[artifacts] will be 0; 5[unlabeled] will be discarded at all times
classification_equalization: False  # for training/classification
clas_scaling_factor: 4  # input scale down factor for training/classification
r_ext_ratio: 1.4  # larger (1.4) for better view under augmentation
r_ext_pixels: 10
# training only
balancing: True
aug_sample_size: 8000
batch_size: 256  # default 256
epochs: 20 
patience: 3  
learning_rate: 0.001  # default 0.001 (Adam)
verbose: 2  # {0, 1, 2}
