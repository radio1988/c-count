configfile: "config.yaml"
import os
from scripts.ccount.snake.input_names import input_names
from scripts.ccount.snake.get_samples import get_samples

DATA_DIR=config["DATA_DIR"]
FORMAT=config["FORMAT"]
WEIGHT=config["WEIGHT"]
WKDIR=os.getcwd()

SAMPLES=get_samples(DATA_DIR)
# I=[1,2,3,4]

rule targets:
    input:
        czi2img=expand("log/img/{s}.done", s=SAMPLES), 
        blob_detection=expand("log/blob_locs/{s}.done", s=SAMPLES),
        blob_cropping=input_names(prefix="res/blob_crops/", SAMPLES=SAMPLES, 
                                  suffix='.crops.npy.gz'),
        classification=input_names(prefix='res/classification1/', SAMPLES=SAMPLES,
                                   suffix=".crops.clas.txt"),
        filter_crops=input_names(prefix='res/classification1/pos/', SAMPLES=SAMPLES,
                                 suffix=".crops.clas.npy.gz"),
        count_file='res/COUNT.csv',
        view_clas_on_image=input_names(prefix="res/classification1/", SAMPLES=SAMPLES,
                                       suffix=".crops.clas.npy.gz.jpg")

#        view0=input_names(SAMPLES),
#        view1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/view/"),
        # area1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/area/",
        #     suffix=".area.txt"),
        # count1="res/COUNT.csv",  # count from weight1
        # area1_agg="res/areas.csv",

rule czi2img:
    input:
        "data/{s}.czi"
    output:
        touch("log/img/{s}.done")
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000  # ~10.5G for '2019'
    log:
        "log/img/{s}.log"
    benchmark:
        "log/img/{s}.benchmark"
    shell:
        """
        python workflow/scripts/czi2img.py -i {input} -c config.yaml -odir res/img &> {log}
        """
    
rule blob_detection:
    input:
        "data/{s}.czi"
    output:
        touch("log/blob_locs/{s}.done"), 
        #input_names(SAMPLES=[{s}], prefix="res/blob_locs/", suffix='.locs.npy.gz')
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000  # ~12G for '2019'
    log:
        "log/blob_locs/{s}.log"
    benchmark:
        "log/blob_locs/{s}.benchmark"
    shell:
        """
        # todo: dynamic config.fname
        python workflow/scripts/blob_detection.py \
        -i {input} -c config.yaml -odir res/blob_locs &> {log}  
        """

rule blob_cropping:
    input:
        czi='data/{s}.czi',
        blob_locs_flag="log/blob_locs/{s}.done"
    output:
        'res/blob_crops/{s}.{i}.crops.npy.gz'
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000 # todo: how much
    log:
        'log/blob_crops/{s}.{i}.crops.npy.gz.log'
    benchmark:
        'log/blob_crops/{s}.{i}.crops.npy.gz.benchmark'
    shell:
        """
        python workflow/scripts/blob_cropping.py -czi {input.czi} \
        -locs res/blob_locs/{wildcards.s}.{wildcards.i}.locs.npy.gz \
        -i {wildcards.i}  -config config.yaml -o {output} &> {log}
        """

rule classification:
    input:
        blob_crops='res/blob_crops/{s}.{i}.crops.npy.gz',
        weight=WEIGHT
    output:
        npy=temp("res/classification1/{s}.{i}.crops.clas.npy.gz"),
        txt="res/classification1/{s}.{i}.crops.clas.txt"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    log:
        "log/classification1/{s}.{i}.log" # todo: log deleted if job fail
    benchmark:
        "log/classification1/{s}.{i}.benchmark"
    shell:
        """
        python workflow/scripts/classification.py  \
        -crops {input.blob_crops} -weight {input.weight} \
        -config config.yaml -odir res/classification1 &> {log}
        """


rule aggr_count:
    input:
        input_names(SAMPLES=SAMPLES, prefix="res/classification1/", suffix=".crops.clas.txt")
    output:
        "res/COUNT.csv"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    priority:
        100
    log:
        "log/COUNT.csv.log"
    shell:
        """
        python workflow/scripts/aggr_count.py {input} {output} &> {log}
        """

rule filter_crops:
    input:
        "res/classification1/{s}.{i}.crops.clas.npy.gz"
    output:
        "res/classification1/pos/{s}.{i}.crops.clas.npy.gz"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    log:
        "log/classification1/pos/{s}.{i}.crops.clas.npy.gz.log"
    benchmark:
        "log/classification1/pos/{s}.{i}.crops.clas.npy.gz.benchmark"
    shell:
        """
        python workflow/scripts/crops_filtering.py -crops {input} \
        -label 1 -output {output} &> {log}
        """

rule view_clas_on_image:
    input:
        crop="res/classification1/{s}.{i}.crops.clas.npy.gz",
        czi="data/{s}.czi"
    output:
        "res/classification1/{s}.{i}.crops.clas.npy.gz.jpg"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000
    log:
        "log/classification1/{s}.{i}.crops.clas.npy.gz.jpg.log"
    benchmark:
        "log/classification1/{s}.{i}.crops.clas.npy.gz.jpg.benchmark"
    shell:
        """
        python workflow/scripts/visualize_locs_on_czi.py \
        -crops {input.crop} -index {wildcards.i} \
        -czi {input.czi} -config config.yaml \
        -output {output} &> {log}
        """

rule area_calculation:
    input:
        "res/classification1/{s}.{i}.clas.npy.gz"
    output:
        txt="res/classification1/area/{s}.{i}.area.txt",
        npy="res/classification1/area/{s}.{i}.area.txt.npy.gz"
    log:
        "res/classification1/area/{s}.{i}.area.log"
    benchmark:
        "res/classification1/area/{s}.{i}.area.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        python workflow/scripts/area_calculation.py {input} {output.txt} &> {log}
        """

rule area_aggregation:
    '''
    Will aggreated all files under res/classification1/area, regardless of config.yaml
    '''
    input:
        input_names(SAMPLES=SAMPLES, 
                    prefix="res/classification1/area/", suffix=".area.txt")
    output:
        "res/areas.csv"
    log:
        "res/areas.csv.log"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    shell:
        """
        python workflow/scripts/aggr_area_info.py res/classification1/area/ res/areas.csv &> {log}
        """

rule view0:
    input:
        "res/blobs/{s}.done"
    output:
        html="res/blobs/view/{s}.{i}.html"
    params:
        html="../res/blobs/view/{s}.{i}.html"
    log:
        "log/blobs/view/{s}.{i}.html.log"
    benchmark:
        "log/blobs/view/{s}.{i}.html.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        fname="res/blobs/{wildcards.s}.{wildcards.i}.npy.gz" dir={WKDIR} \
        jupyter nbconvert --to html --execute workflow/notebooks/viewing_blobs.ipynb \
        --output {params.html} &> {log}
        """

rule view1:
    input:
        "res/classification1/{s}.{i}.clas.npy.gz"  # some will not exist, but just ignore warnings
    output:
        html="res/classification1/view/{s}.{i}.html"
    params:
        html="../res/classification1/view/{s}.{i}.html"
    log:
        "log/classification1/view/{s}.{i}.html.log"
    benchmark:
        "log/classification1/view/{s}.{i}.html.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        mkdir -p res res/classification1 res/classification1/view
        fname={input} dir={WKDIR} \
        jupyter nbconvert --to html --execute workflow/notebooks/viewing_blobs.ipynb \
        --output {params.html} &> {log}
        """
