configfile: "config.yaml"
import os
from scripts.ccount.snake.input_names import input_names
from scripts.ccount.snake.get_samples import get_samples

DATA_DIR=config["DATA_DIR"]
FORMAT=config["FORMAT"]
WEIGHT1=config["WEIGHT1"]
WEIGHT2=config["WEIGHT2"]
WKDIR=os.getcwd()

SAMPLES=get_samples(DATA_DIR)

rule targets:
    input:
        images=expand("log/img/{s}.done", s=SAMPLES),  # czi2img
        blob_locs=expand("log/blob_locs/{s}.done", s=SAMPLES),  # blob_detection
        blob_crops=input_names(prefix="res/blob_crops/", SAMPLES=SAMPLES, 
                               suffix='.crops.npy.gz'),
        count_file='res/COUNT.csv', # requires/enables classification

#        view0=input_names(SAMPLES),
#        view1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/view/"),
        # area1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/area/",
        #     suffix=".area.txt"),
        # count1="res/COUNT.csv",  # count from weight1
        # area1_agg="res/areas.csv",

rule czi2img:
    input:
        "data/{s}.czi"
    output:
        touch("log/img/{s}.done") # if not error
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000  # ~10.5G for '2019'
    log:
        "log/img/{s}.log"
    benchmark:
        "log/img/{s}.benchmark"
    shell:
        """
        python workflow/scripts/czi2img.py -i {input} -c config.yaml -odir res/img &> {log}
        """
    
rule blob_detection:
    input:
        "data/{s}.czi"
    output:
        # touch("log/blob_locs/{s}.done"), 
        dynamic("res/blob_locs/{s}.{i}.locs.npy.gz")
        #input_names(SAMPLES=[{s}], prefix="res/blob_locs/", suffix='.locs.npy.gz')
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000  # ~12G for '2019'
    log:
        "log/blob_locs/{s}.{i}.log"
    benchmark:
        "log/blob_locs/{s}.{i}.benchmark"
    shell:
        """
        # todo: dynamic config.fname
        python workflow/scripts/blob_detection.py \
        -i {input} -c config.yaml -odir res/blob_locs &> {log}  
        """

rule blob_cropping:
    input:
        czi='data/{s}.czi',
        # blob_locs_flag="log/blob_locs/{s}.done", 
        blob_locs=dynamic("res/blob_locs/{s}.{i}.locs.npy.gz")
    output:
        'res/blob_crops/{s}.{i}.crops.npy.gz'
    params:
        blob_locs=input_names(SAMPLES={wildcards.s}, prefix="res/blob_locs/", suffix='.locs.npy.gz')
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000 # todo: how much
    log:
        'log/blob_crops/{s}.{i}.crops.npy.gz.log'
    benchmark:
        'log/blob_crops/{s}.{i}.crops.npy.gz.benchmark'
    shell:
        """
        python workflow/scripts/blob_cropping.py -czi {input.czi} -locs {params.blob_locs} \
        -i {wildcards.i}  -config config.yaml -o {output} &> {log}
        """

rule classification:
    input:
        blob_crops='res/blob_crops/{s}.{i}.crops.npy.gz',
        weight=WEIGHT1
    output:
        npy=temp("res/classification1/{s}.{i}.crops.clas.npy.gz"),
        txt=temp("res/classification1/{s}.{i}.crops.clas.txt")
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    log:
        "log/classification1/{s}.{i}.log" # todo: log deleted if job fail
    benchmark:
        "log/classification1/{s}.{i}.benchmark"
    shell:
        """
        python workflow/scripts/classification.py  \
        -db {input.blob_crops} \
        -o res/classification1 -l 1 -w {input.weight} &> {log}
        """

rule aggr_count:
    input:
        input_names(SAMPLES=SAMPLES, prefix="res/classification1/", suffix=".crops.clas.txt")
    output:
        "res/COUNT.csv"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    priority:
        100
    log:
        "log/COUNT.csv.log"
    shell:
        """
        python workflow/scripts/aggr_count.py {input} {output} &> {log}
        """

rule area_calculation:
    input:
        "res/classification1/{s}.{i}.clas.npy.gz"
    output:
        txt="res/classification1/area/{s}.{i}.area.txt",
        npy="res/classification1/area/{s}.{i}.area.txt.npy.gz"
    log:
        "res/classification1/area/{s}.{i}.area.log"
    benchmark:
        "res/classification1/area/{s}.{i}.area.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        python workflow/scripts/area_calculation.py {input} {output.txt} &> {log}
        """

rule area_aggregation:
    '''
    Will aggreated all files under res/classification1/area, regardless of config.yaml
    '''
    input:
        input_names(SAMPLES=SAMPLES, 
                    prefix="res/classification1/area/", suffix=".area.txt")
    output:
        "res/areas.csv"
    log:
        "res/areas.csv.log"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    shell:
        """
        python workflow/scripts/aggr_area_info.py res/classification1/area/ res/areas.csv &> {log}
        """

rule view0:
    input:
        "res/blobs/{s}.done"
    output:
        html="res/blobs/view/{s}.{i}.html"
    params:
        html="../res/blobs/view/{s}.{i}.html"
    log:
        "log/blobs/view/{s}.{i}.html.log"
    benchmark:
        "log/blobs/view/{s}.{i}.html.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        fname="res/blobs/{wildcards.s}.{wildcards.i}.npy.gz" dir={WKDIR} \
        jupyter nbconvert --to html --execute workflow/notebooks/viewing_blobs.ipynb \
        --output {params.html} &> {log}
        """

rule view1:
    input:
        "res/classification1/{s}.{i}.clas.npy.gz"  # some will not exist, but just ignore warnings
    output:
        html="res/classification1/view/{s}.{i}.html"
    params:
        html="../res/classification1/view/{s}.{i}.html"
    log:
        "log/classification1/view/{s}.{i}.html.log"
    benchmark:
        "log/classification1/view/{s}.{i}.html.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        mkdir -p res res/classification1 res/classification1/view
        fname={input} dir={WKDIR} \
        jupyter nbconvert --to html --execute workflow/notebooks/viewing_blobs.ipynb \
        --output {params.html} &> {log}
        """
