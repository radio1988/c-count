configfile: "config.yaml"
import os
from damid_modules import input_names, get_samples

DATA_DIR=config["DATA_DIR"]
FORMAT=config["FORMAT"]
WEIGHT1=config["WEIGHT1"]
WEIGHT2=config["WEIGHT2"]
WKDIR=os.getcwd()

SAMPLES=get_samples(DATA_DIR)

rule targets:
    input:
        img=expand("res/img/log/{s}.finished", s=SAMPLES),  # czi2img
        blobs=expand("res/blobs/{s}.finished", s=SAMPLES),  # blob_detection
        view0=input_names(SAMPLES),
        view1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/view/"),
        area1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/area/",
            suffix=".area.txt"),
        count1="res/COUNT.csv",  # count from weight1
        area1_agg="res/areas.csv",

rule czi2img:
    input:
        "data/{s}.czi"
    output:
        "res/img/jpg/{s}.0.jpg",
        "res/img/equ/{s}.0.equ.jpg", # only tests if image of the first area
        touch("res/img/log/{s}.finished") # if not error
    priority: 
        0
    threads:
        1
    log:
        "res/img/log/{s}.log"
    benchmark:
        "res/img/log/{s}.tsv"
    params:
        mem='40000'
    shell:
        """
        python workflow/czi2img.py -i {input} -f {FORMAT} -odir res/img &> {log}
        """
    
rule blob_detection:
    input:
        "data/{s}.czi"
    output:
        "res/blobs/{s}.0.npy.gz", # todo: include all outputs
        touch("res/blobs/{s}.finished")
    priority:
        50
    threads:
        1
    log:
        "res/blobs/{s}.log"
    benchmark:
        "res/blobs/{s}.tsv"
    params:
        mem="45000"
    shell:
        """
        python workflow/blob_detection.py -i {input} -f {FORMAT} -odir res/blobs/ &> {log}
        """

rule view0:
    input:
        "res/blobs/{s}.finished"
    output:
        nb="res/blobs/view/{s}.{i}.ipynb",
        html="res/blobs/view/{s}.{i}.html"
    log:
        "res/blobs/view/{s}.{i}.html.log"
    benchmark:
        "res/blobs/view/{s}.{i}.html.tsv"
    threads:
        1
    params:
        mem="8000"
    shell:
        """
        fname="res/blobs/{wildcards.s}.{wildcards.i}.npy.gz" dir={WKDIR} runipy workflow/viewing_blobs.ipynb \
        {output.nb} &> {log}
        jupyter nbconvert --to html {output.nb} &>> {log}
        """

rule classification1:
    input:
        blobs="res/blobs/{s}.finished",
        weight=WEIGHT1
    output:
        "res/classification1/{s}.{i}.pred.npy.gz",
    log:
        "res/classification1/{s}.{i}.log"
    benchmark:
        "res/classification1/{s}.{i}.tsv"
    threads:
        1
    params:
        mem="8000"
    shell:
        """
        python workflow/classification.pred.py  -db "res/blobs/{wildcards.s}.{wildcards.i}.npy.gz" \
        -o res/classification1 -l 1 -w {input.weight} &> {log}
        """

rule view1:
    input:
        "res/classification1/{s}.{i}.pred.npy.gz"  # some will not exist, but just ignore warnings
    output:
        nb="res/classification1/view/{s}.{i}.ipynb",
        html="res/classification1/view/{s}.{i}.html"
    log:
        "res/classification1/view/{s}.{i}.html.log"
    benchmark:
        "res/classification1/view/{s}.{i}.html.tsv"
    threads:
        1
    params:
        mem="8000"
    shell:
        """
        mkdir -p res res/classification1 res/classification1/view
        fname={input} dir={WKDIR} runipy workflow/viewing_blobs.ipynb {output.nb} &> {log}
        jupyter nbconvert --to html {output.nb} &>> {log}
        """

rule count1:
    input:
        input_names(SAMPLES=SAMPLES, prefix="res/classification1/", suffix=".pred.npy.gz")
    output:
        raw="res/classification1/COUNT1.txt",
        clean="res/COUNT.csv"
    threads:
        1
    params:
        mem="1000"
    priority:
        100
    log:
        "res/COUNT.csv.log"
    shell:
        """
        grep 'Predictions' {input} > {output.raw} 2> {log}
        python workflow/get_counts.py res/classification1/COUNT1.txt res/COUNT.csv &>> {log}
        """

rule area_calculation1:
    input:
        "res/classification1/{s}.{i}.pred.npy.gz"
    output:
        "res/classification1/area/{s}.{i}.area.txt"
    log:
        "res/classification1/area/{s}.{i}.area.log"
    benchmark:
        "res/classification1/area/{s}.{i}.area.tsv"
    threads:
        1
    params:
        mem="8000"
    shell:
        """
        python workflow/area_calculation.py {input} 1 {output} &> {log}
        """


rule area_aggregation:
    input:
        input_names(SAMPLES=SAMPLES, 
                    prefix="res/classification1/area/", suffix=".area.txt")
    output:
        "res/areas.csv"
    log:
        "res/areas.csv.log"
    threads:
        1
    params:
        mem="1000"
    shell:
        """
        python workflow/get_areas.py res/classification1/area/ res/areas.csv &> {log}
        """

# rule filter2:
#     input:
#         "res/classification1/{s}.yes.npy.gz"
#     output:
#         yes="res/filter2/{s}.yes.yes.npy.gz",
#         all="res/filter2/{s}.yes.pred.npy.gz",
#         log="res/filter2/{s}.log"
#     benchmark:
#         "res/filter2/{s}.tsv"
#     threads:
#         1
#     params:
#         mem="8000"
#     shell:
#         """
#         python workflow/classification.py  -db {input} -o res/filter2 -l 1 -w {WEIGHT2} &> {output.log}
#         """



# rule count2:
#     input:
#         expand("res/filter2/{s}.log", s=SAMPLES)
#     output:
#         "res/filter2/COUNT2.txt"
#     threads:
#         1
#     params:
#         mem="1000"
#     shell:
#         """
#         grep 'Predictions' {input} > {output}
#         """



# rule view2:
#     input:
#         "res/filter2/{s}.yes.pred.npy.gz"
#     output:
#         nb="res/filter2/{s}.ipynb",
#         html="res/filter2/{s}.html"
#     log:
#         "res/filter2/{s}.html.log"
#     benchmark:
#         "res/filter2/{s}.html.tsv"
#     threads:
#         1
#     params:
#         mem="8000"
#     shell:
#         """
#         fname={input} dir={WKDIR} runipy workflow/viewing_blobs.ipynb {output.nb} &> {log}
#         jupyter nbconvert --to html {output.nb} &>> {log}
#         """


