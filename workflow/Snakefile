configfile: "config.yaml"
import os
from scripts.ccount.snake.input_names import input_names
from scripts.ccount.snake.get_samples import get_samples

DATA_DIR=config["DATA_DIR"]
FORMAT=config["FORMAT"]
WEIGHT1=config["WEIGHT1"]
WEIGHT2=config["WEIGHT2"]
WKDIR=os.getcwd()

SAMPLES=get_samples(DATA_DIR)

rule targets:
    input:
        img=expand("log/img/{s}.finished", s=SAMPLES),  # czi2img
        blobs=expand("log/blobs/{s}.finished", s=SAMPLES),  # blob_detection
#        view0=input_names(SAMPLES),
#        view1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/view/"),
        # area1=input_names(SAMPLES=SAMPLES, prefix="res/classification1/area/",
        #     suffix=".area.txt"),
        # count1="res/COUNT.csv",  # count from weight1
        # area1_agg="res/areas.csv",

rule czi2img:
    input:
        "data/{s}.czi"
    output:
        touch("log/img/{s}.finished") # if not error
    threads:
        1
    log:
        "log/img/{s}.log"
    benchmark:
        "log/img/{s}.benchmark"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000  # ~10.5G for '2019'
    shell:
        """
        python workflow/scripts/czi2img.py -i {input} -c config.yaml -odir res/img &> {log}
        """
    
rule blob_detection:
    input:
        "data/{s}.czi"
    output:
        touch("log/blobs/{s}.finished")
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000  # ~12G for '2019'
    log:
        "log/blobs/{s}.log"
    benchmark:
        "log/blobs/{s}.benchmark"
    shell:
        """
        # todo: dynamic config.fname
        python workflow/scripts/blob_detection.py \
        -i {input} -c config.yaml -odir res/blob_locs &> {log}  
        """

# rule blob_cropping:
#     input:
#         "log/blobs/{s}.finished"
#     output:
#         "res/blob_croppings/{s}.croppings.npy.gz"
#     shell:
#         "cp {input} {output}"

rule view0:
    input:
        "res/blobs/{s}.finished"
    output:
        html="res/blobs/view/{s}.{i}.html"
    params:
        html="../res/blobs/view/{s}.{i}.html"
    log:
        "log/blobs/view/{s}.{i}.html.log"
    benchmark:
        "log/blobs/view/{s}.{i}.html.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        fname="res/blobs/{wildcards.s}.{wildcards.i}.npy.gz" dir={WKDIR} \
        jupyter nbconvert --to html --execute workflow/notebooks/viewing_blobs.ipynb \
        --output {params.html} &> {log}
        """

rule classification1:
    input:
        blobs="log/blobs/{s}.finished",
        weight=WEIGHT1
    output:
        npy=temp("res/classification1/{s}.{i}.pred.npy.gz"),
        log="res/classification1/{s}.{i}.log"
    benchmark:
        "res/classification1/{s}.{i}.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        python workflow/scripts/classification.pred.py  \
        -db "res/blobs/{wildcards.s}.{wildcards.i}.npy.gz" \
        -o res/classification1 -l 1 -w {input.weight} &> {output.log}
        """

rule view1:
    input:
        "res/classification1/{s}.{i}.pred.npy.gz"  # some will not exist, but just ignore warnings
    output:
        html="res/classification1/view/{s}.{i}.html"
    params:
        html="../res/classification1/view/{s}.{i}.html"
    log:
        "log/classification1/view/{s}.{i}.html.log"
    benchmark:
        "log/classification1/view/{s}.{i}.html.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        mkdir -p res res/classification1 res/classification1/view
        fname={input} dir={WKDIR} \
        jupyter nbconvert --to html --execute workflow/notebooks/viewing_blobs.ipynb \
        --output {params.html} &> {log}
        """

rule count1:
    input:
        input_names(SAMPLES=SAMPLES, prefix="res/classification1/", suffix=".log")
    output:
        raw="res/classification1/COUNT1.txt",
        clean="res/COUNT.csv"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    priority:
        100
    log:
        "res/COUNT.csv.log"
    shell:
        """
        grep 'Predictions' {input} > {output.raw} 2> {log}
        python workflow/scripts/get_counts.py res/classification1/COUNT1.txt res/COUNT.csv &>> {log}
        """

rule area_calculation1:
    input:
        "res/classification1/{s}.{i}.pred.npy.gz"
    output:
        txt="res/classification1/area/{s}.{i}.area.txt",
        npy="res/classification1/area/{s}.{i}.area.txt.npy.gz"
    log:
        "res/classification1/area/{s}.{i}.area.log"
    benchmark:
        "res/classification1/area/{s}.{i}.area.benchmark"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000
    shell:
        """
        python workflow/scripts/area_calculation.py {input} {output.txt} &> {log}
        """


rule area_aggregation:
    '''
    Will aggreated all files under res/classification1/area, regardless of config.yaml
    '''
    input:
        input_names(SAMPLES=SAMPLES, 
                    prefix="res/classification1/area/", suffix=".area.txt")
    output:
        "res/areas.csv"
    log:
        "res/areas.csv.log"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    shell:
        """
        python workflow/scripts/get_areas.py res/classification1/area/ res/areas.csv &> {log}
        """

# rule filter2:
#     input:
#         "res/classification1/{s}.yes.npy.gz"
#     output:
#         yes="res/filter2/{s}.yes.yes.npy.gz",
#         all="res/filter2/{s}.yes.pred.npy.gz",
#         log="res/filter2/{s}.log"
#     benchmark:
#         "res/filter2/{s}.benchmark"
#     threads:
#         1
#     params:
#         mem="8000"
#     shell:
#         """
#         python workflow/classification.py  \
#          -db {input} -o res/filter2 -l 1 -w {WEIGHT2} &> {output.log}
#         """



# rule count2:
#     input:
#         expand("res/filter2/{s}.log", s=SAMPLES)
#     output:
#         "res/filter2/COUNT2.txt"
#     threads:
#         1
#     params:
#         mem="1000"
#     shell:
#         """
#         grep 'Predictions' {input} > {output}
#         """



# rule view2:
#     input:
#         "res/filter2/{s}.yes.pred.npy.gz"
#     output:
#         nb="res/filter2/{s}.ipynb",
#         html="res/filter2/{s}.html"
#     log:
#         "res/filter2/{s}.html.log"
#     benchmark:
#         "res/filter2/{s}.html.benchmark"
#     threads:
#         1
#     params:
#         mem="8000"
#     shell:
#         """
#         fname={input} dir={WKDIR} runipy workflow/viewing_blobs.ipynb {output.nb} &> {log}
#         jupyter nbconvert --to html {output.nb} &>> {log}
#         """


