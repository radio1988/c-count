# C-COUNT

colony count

# Installation
## Dec 2021
- install mini-conda: https://docs.conda.io/en/latest/miniconda.html
- install mamba: https://anaconda.org/conda-forge/mamba 
- install ccount-env: `mamba env create -f workflow/env/ccount.yaml`
- update ccount-env: `conda env update -n ccount -f workflow/env/ccount.yaml`

# Usage
## Counting workflow (Nov. 30, 2020)
- mkdir $workdir && cd $workdir
- ln -s $path_data data
- ln -s $path_ccount/workflow
- conda activate ccount-env
- cp workflow/config.yaml ./ && vim config.yaml (edit config.yaml)
- cp workflow/config.data_curve.yaml ./ && vim config.data_curve.yaml 
- `snakemake -pk -j 1` or `sh submit.sh` (on HPC) 

<<<< Editted Dec, 2021


## Labeling workflow (Aug. 2020)
- copy ccount.py and labeling.ipynb into the working dir, alone with a xxx.npy.gz file containing the detected blobs
- source activate ccount
- jupyter-notebook
- open labeling.ipynb and work from there

## Description of scripts
- czi2npy (blob_detection.py, edge filter included)
- czi2img (czi2img.py)
- view0 of all blobs detected
- filter_merge
- labeling
- classification

## Usage Notes
- start jupyter-notebook and work from there (filter_merge, labeling)
- work from terminal (classification)
- each blob_detection takes 18G RAM

##### FOR INTERNAL USE ####
ccount will input raw image (czi files), and output colony counts, and colony size distribution.

Notations:
- plate: each plate loaded with cells and colonies
- image: each czi image that is generated by the microscope, usually combining one or several areas from the plate
- scanned-area (area): each scanned area wrapped in a czi image, sometimes there are four areas ("Top", "Left", "Right", "Bottom") in a plate, sometimes there is only one area
- In this readme, pound sign (#) will be used to comment on the code demonstrated here, so that the workstation will not confuse them with code
- sign ` encloses code to be used

Analysis steps:
- connect to workstation via ssh (terminal interface):
	- `ssh -L3333:localhost:8888  socolovsky_lab@146.189.48.234`
	- type in the password (hint: m.....@)
- change work directory to ./ccount/analysis and create a subfolder under ./ccount/analysis
	- `cd ./ccount/analysis`
	- `mkdir {date_name}, e.g. 202007_Ashley_Epo_DRC_17JUL20`
	- `cd 202007_Ashley_Epo_DRC_17JUL20`
	- $workdir is ./ccount/analysis/202007_Ashley_Epo_DRC_17JUL20/  now
- setup working environment and running the analysis workflow 
	- make sure you are in $workdir by checking with the command `pwd`
	- `ln -s ~/ccount/ccount/workflow`  # get the ccount workflow scripts in $workdir
	- `ln -s ~/ccount/ccount_data/Ashley_Epo_DRC_17JUL20/ data` # create a link to dataset in $workdir; this data folder should contain xxx.czi files; file names should not contain spaces, and only contain alphabets, numbers, underscore (_) or hyphen (-), file names should not contain 'Top', 'Left', 'Right', 'Bottom', unless your image contains only one area, and the corresponding area, e.g. the Top of the plate. When these keywords are used, only one area is analyzed from the image data, using these keywords by mistake will result in problems.
	- `cp ~/ccount/ccount/config.yaml ./ ` # copy the configuration file to $workdir, if standard image format, no need to edit this file; if special format used, or data are not FL (fetal liver), contact Rui for how to edit the file, and edit it with a simple text editor, e.g. 
	- `cp -r ~/ccount/ccount/weights ./ `  # copy the pre-trained CNN model to $workdir
	- `conda activate ccount`  # activate the conda environment called 'ccount', basically enabling necessary python libraries
	- `snakemake -p -k -j 1 -n`   # 'dry-run' to check if all configurations are correct, without actually running the workflow
	- `nohup snakemake -p -k -j 1 &> log.txt & ` # running the workflow and getting data analyzed. the nohup command will make sure the workflow keeps running even if your ssh session got disconnected, e.g. your laptop become offline
- Description of outputs
	- all results are under $workdir/res/ folder
	- COUNT.txt shows the number of colonies in each area of the plate. If there are four areas, they will be numbered 0, 1, 2, 3. If there is only one area, they will be numbered as 0 in it's filename
	- areas.csv shows the size of each colony for each area, the size are in number of pixels. We might rename this file to sizes.csv  in the future
	- folder img/ contains the raw images and equalized images in jpg format
	- folder blobs/ contains the intermediate results of the blob_detection step 
		- subfolder vis visualized the blob detection performance for performance check, yellow circles shows blobs detected
		- subfolder view contains html files showing a few randomly sampled blobs being detected in details, some of the blobs will be cells, debris, imaging artifacts, most of them will be labeled as 'False' in the classification step
		- subfolder hist shows the radius distribution  for all blobs in the blob_detection step. This is for trouble shooting only, not useful for biological usage. Please use areas.csv for colony sizes. As they are not accurate, and non-colony blobs included.
	- folder classification1 contains the intermediate results of the classification step
		- subfolder view contains html files showing random examples of blob classification. Useful to trouble-shoot/quality-check the classification step
		- subfolder hist contain PDF files that are histograms showing the size distribution of all areas in all plates.

- FAQ and trouble shoot:
	- Limitations:
		- currently(Jan, 2021), only one analysis workflow can be run at the same time, as some of the analysis steps takes about ~30GB of RAM, and the total RAM on the workstation is 32GB
	- How to check how many percentage of my analysis are finished?
		- read log.txt under workdir
	- If the workflow stopped unexpectedly (e.g. two person ran at the same time and workstation our of RAM), how to re-run?
		- just ssh again, change work directory (wkdir) to the correct path, dry-run again to check the configuration and re-run
			- `ssh -L3333:localhost:8888  socolovsky_lab@146.189.48.234`
			- `cd ./ccount/analysis/202007_Ashley_Epo_DRC_17JUL20`
			- `conda activate ccount`
			- `snakemake -p -k -j 1 -n  `
			- `snakemake -j 1 --unlock`  # Use this command only if an error code of 'Please UNLOCK' the workdir is shown in previous command, and you're sure you've killed the old workflow (snakemake) process. The tricky part is, if you 'unlock' without killing the previous workflow, two workflows will run on the same dataset and same workdir at the same time and cause lots of problems. 
			- `snakemake -p -k -j 1 -n`
			- `nohup snakemake -p -k -j 1 &> log.txt &`
	- If you found error in configuration, e.g. wrong dataset selected, wrong config.yaml, wrong weight, and you want to stop the workflow from running
		- if you are still connected to the same ssh session (have not logged off yet), simply type fg to show the workflow being run in the background, and ctrl + C to kill the workflow process 
		- if you have disconnected and re-connected, the above method will not work, you can: 
			- type 'top|grep snakemake', see something like '13519 socolov+  20   0  501032  39180  37072 S   0.0  0.1   0:01.03 snakemake'
			- type 'kill -9 13519' to kill the workflow process, 13519 is the job-ID
			- type 'top' and kill the last job submitted by the snakemake workflow, e.g. a python process taking 100% of the CPU. This is a bit tricky. If can't figure this out, simply wait for 1 hour before re-submitting a workflow, most old processes will be finished in 1 hour
